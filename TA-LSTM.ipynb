{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\__init__.py:1755\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m \n\u001b[0;32m   1754\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 1755\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _StorageBase\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\functional.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lowrank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m svd_lowrank, pca_lowrank\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     13\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[0;32m     14\u001b[0m     handle_torch_function)\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\nn\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      6\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\nn\\modules\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1d, Conv2d, Conv3d, \\\n\u001b[0;32m      4\u001b[0m     ConvTranspose1d, ConvTranspose2d, ConvTranspose3d, \\\n\u001b[0;32m      5\u001b[0m     LazyConv1d, LazyConv2d, LazyConv3d, LazyConvTranspose1d, LazyConvTranspose2d, LazyConvTranspose3d\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Threshold, ReLU, Hardtanh, ReLU6, Sigmoid, Tanh, \\\n\u001b[0;32m      7\u001b[0m     Softmax, Softmax2d, LogSoftmax, ELU, SELU, CELU, GELU, Hardshrink, LeakyReLU, LogSigmoid, \\\n\u001b[0;32m      8\u001b[0m     Softplus, Softshrink, MultiheadAttention, PReLU, Softsign, Softmin, Tanhshrink, RReLU, GLU, \\\n\u001b[0;32m      9\u001b[0m     Hardsigmoid, Hardswish, SiLU, Mish\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\nn\\modules\\linear.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parameter, UninitializedParameter\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\nn\\functional.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# The JIT doesn't understand Union, nor torch.dtype here\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     DType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_jit_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverrides\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     has_torch_function, has_torch_function_unary, has_torch_function_variadic,\n\u001b[0;32m     28\u001b[0m     handle_torch_function)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _reduction \u001b[38;5;28;01mas\u001b[39;00m _Reduction\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\_jit_internal.py:46\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# This is needed. `torch._jit_internal` is imported before `torch.distributed.__init__`.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Explicitly ask to import `torch.distributed.__init__` first.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Otherwise, \"AttributeError: module 'torch' has no attribute 'distributed'\" is raised.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mangling\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpackage_mangling\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_awaits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Await\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Await \u001b[38;5;28;01mas\u001b[39;00m CAwait, Future \u001b[38;5;28;01mas\u001b[39;00m CFuture\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\package\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyze\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mis_from_package\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_from_package\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_structure_representation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Directory\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglob_group\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobGroup\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\package\\analyze\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfind_first_use_of_broken_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_first_use_of_broken_modules\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrace_dependencies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trace_dependencies\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\package\\analyze\\find_first_use_of_broken_modules.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackage_exporter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PackagingError\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind_first_use_of_broken_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_first_use_of_broken_modules\u001b[39m(exc: PackagingError) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]:\n",
      "File \u001b[1;32mc:\\Users\\Satrajit Ghosh\\.conda\\envs\\MachineLearning\\lib\\site-packages\\torch\\package\\package_exporter.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_digraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DiGraph\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_importlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _normalize_path\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mangling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m demangle, is_mangled\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_package_pickler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_pickler\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stdlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_stdlib_module\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:971\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:914\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1407\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1379\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1539\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:156\u001b[0m, in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:148\u001b[0m, in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, attention_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, attention_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(attention_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states shape: (batch_size, seq_len, hidden_dim)\n",
    "        attention_weights = self.attention(hidden_states)  # (batch_size, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)  # (batch_size, seq_len, 1)\n",
    "        attended = torch.bmm(hidden_states.transpose(1, 2), attention_weights)  # (batch_size, hidden_dim, 1)\n",
    "        attended = attended.squeeze(-1)  # (batch_size, hidden_dim)\n",
    "        return attended, attention_weights\n",
    "\n",
    "class TALSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TALSTM, self).__init__()\n",
    "        \n",
    "        # Dimensions for the model\n",
    "        self.video_feature_dim = 32  # Adjusted based on your conv output\n",
    "        self.audio_feature_dim = 256  # Adjusted based on your conv output\n",
    "        self.lstm_hidden_dim = 256\n",
    "        self.attention_dim = 128\n",
    "        \n",
    "        # Video Processing Branch\n",
    "        self.video_conv = nn.Sequential(\n",
    "            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)),\n",
    "            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2))\n",
    "        )\n",
    "        \n",
    "        # Audio Processing Branch\n",
    "        self.audio_conv = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        # LSTM and Attention Layers\n",
    "        self.video_lstm = nn.LSTM(\n",
    "            self.video_feature_dim, \n",
    "            self.lstm_hidden_dim, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.video_attention = TemporalAttention(\n",
    "            self.lstm_hidden_dim * 2, \n",
    "            self.attention_dim\n",
    "        )\n",
    "        \n",
    "        self.audio_lstm = nn.LSTM(\n",
    "            self.audio_feature_dim, \n",
    "            self.lstm_hidden_dim, \n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.audio_attention = TemporalAttention(\n",
    "            self.lstm_hidden_dim * 2, \n",
    "            self.attention_dim\n",
    "        )\n",
    "        \n",
    "        # Fusion and Output Layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.lstm_hidden_dim * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(256, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "        # Process video through CNN\n",
    "        batch_size = video.size(0)\n",
    "        video_features = self.video_conv(video)\n",
    "        video_features = video_features.view(batch_size, self.video_feature_dim, -1)\n",
    "        video_features = video_features.transpose(1, 2)  # (batch_size, seq_len, features)\n",
    "        \n",
    "        # Process audio through CNN\n",
    "        audio_features = self.audio_conv(audio)\n",
    "        audio_features = audio_features.transpose(1, 2)  # (batch_size, seq_len, features)\n",
    "        \n",
    "        # Process through LSTM and Attention\n",
    "        video_lstm_out, _ = self.video_lstm(video_features)\n",
    "        video_attended, video_attention = self.video_attention(video_lstm_out)\n",
    "        \n",
    "        audio_lstm_out, _ = self.audio_lstm(audio_features)\n",
    "        audio_attended, audio_attention = self.audio_attention(audio_lstm_out)\n",
    "        \n",
    "        # Fuse features\n",
    "        fused_features = torch.cat([video_attended, audio_attended], dim=1)\n",
    "        fused_features = self.fusion(fused_features)\n",
    "        \n",
    "        # Final output\n",
    "        output = self.output_layer(fused_features)\n",
    "        \n",
    "        return output, video_attention, audio_attention\n",
    "\n",
    "# Modified training function to handle attention outputs\n",
    "def train_talstm(model, train_loader, val_loader, num_epochs=30, device=\"cpu\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for videos, audios, labels in train_progress:\n",
    "            videos = videos.to(device)\n",
    "            audios = audios.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, video_attention, audio_attention = model(videos, audios)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_progress.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_attention_video = []\n",
    "        val_attention_audio = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_progress = tqdm(val_loader, desc=\"Validation\")\n",
    "            for videos, audios, labels in val_progress:\n",
    "                videos = videos.to(device)\n",
    "                audios = audios.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs, video_attention, audio_attention = model(videos, audios)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_progress.set_postfix({'loss': loss.item()})\n",
    "                \n",
    "                # Store attention weights for analysis\n",
    "                val_attention_video.append(video_attention.cpu().numpy())\n",
    "                val_attention_audio.append(audio_attention.cpu().numpy())\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {total_train_loss / len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {total_val_loss / len(val_loader):.4f}\")\n",
    "    \n",
    "    return model, val_attention_video, val_attention_audio\n",
    "\n",
    "# Usage example\n",
    "def initialize_and_train():\n",
    "    # Initialize model\n",
    "    model = TALSTM()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, video_attention, audio_attention = train_talstm(\n",
    "        model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        num_epochs=30, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    return trained_model, video_attention, audio_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()  # For phoneme classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for video, audio, phoneme_labels in train_loader:  # Load synced video/audio pairs\n",
    "        video, audio, phoneme_labels = video.to(device), audio.to(device), phoneme_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _, _ = model(video, audio)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, phoneme_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop with mismatched pairs\n",
    "model.eval()\n",
    "mismatched_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for video, audio, _ in test_loader:\n",
    "        video, audio = video.to(device), audio.to(device)\n",
    "\n",
    "        # Create mismatched pairs\n",
    "        shuffled_audio = audio[torch.randperm(audio.size(0))]  # Random shuffle of audio embeddings\n",
    "\n",
    "        # Get predictions\n",
    "        matched_outputs, _, _ = model(video, audio)  # Matched pair\n",
    "        mismatched_outputs, _, _ = model(video, shuffled_audio)  # Mismatched pair\n",
    "\n",
    "        # Store predictions\n",
    "        matched_predictions = torch.argmax(matched_outputs, dim=1).cpu().numpy()\n",
    "        mismatched_predictions = torch.argmax(mismatched_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        mismatched_results.append((matched_predictions, mismatched_predictions))\n",
    "\n",
    "# Analyze results\n",
    "for idx, (matched, mismatched) in enumerate(mismatched_results):\n",
    "    print(f\"Sample {idx}: Matched: {matched}, Mismatched: {mismatched}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot attention weights\n",
    "def plot_attention(attention_weights, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(attention_weights.cpu().numpy(), cmap='viridis', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Batch Index')\n",
    "    plt.show()\n",
    "\n",
    "# Example for one batch\n",
    "_, video_attention_weights, audio_attention_weights = model(video, shuffled_audio)\n",
    "plot_attention(video_attention_weights[0], \"Video Attention (Mismatched)\")\n",
    "plot_attention(audio_attention_weights[0], \"Audio Attention (Mismatched)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
